{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "deeb18f8-574f-4070-9f17-82141179f246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# dim_account_gld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8f71e31-f20c-491f-ab21-4eff90baf75c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"workspace\"\n",
    "SCHEMA  = \"ai_bi_reporting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a260d20-cc62-402f-9394-1f1ce9fe3069",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "acct_slv = f\"{CATALOG}.{SCHEMA}.anon_bst_acct_slv\"\n",
    "dim_acct = f\"{CATALOG}.{SCHEMA}.dim_account_gld\"\n",
    "\n",
    "dfa = spark.table(acct_slv)\n",
    "\n",
    "# Keep curated columns only (extend as needed)\n",
    "keep = [c for c in dfa.columns if c in {\n",
    "    \"Id\",\"Name\",\"Type\",\"AccountSource\",\"FY_Revenue_Target\",\"CVA_NPS\",\"CVA_CSAT\"\n",
    "}]\n",
    "dfa = dfa.select(*keep)\n",
    "\n",
    "# Optional: create a surrogate key (hash)\n",
    "dfa = dfa.withColumn(\n",
    "    \"account_sk\",\n",
    "    F.sha2(F.col(\"Id\").cast(\"string\"), 256)\n",
    ")\n",
    "\n",
    "(\n",
    "    dfa.write\n",
    "    .option(\"delta.columnMapping.mode\", \"name\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(dim_acct)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbf4c81c-5f7b-4e29-9216-3102a23f97ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# dim_service_gld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00552540-248f-4c55-8986-38ddea0ea66e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cust_slv = f\"{CATALOG}.{SCHEMA}.anon_cust_intelligence_opportunity_slv\"\n",
    "dim_srv = f\"{CATALOG}.{SCHEMA}.dim_service_gld\"\n",
    "\n",
    "dfs = spark.table(cust_slv).select(\"Service Family\",\"Service Line\").dropDuplicates()\n",
    "\n",
    "# Surrogate key for the service grain\n",
    "dfs = dfs.withColumn(\"service_sk\", F.sha2(F.concat_ws(\"||\", *[F.col(\"Service Family\"), F.col(\"Service Line\")]), 256))\n",
    "\n",
    "(dfs.write\n",
    "    .option(\"delta.columnMapping.mode\",\"name\")\n",
    "    .option(\"overwriteSchema\",\"true\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(dim_srv))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fac07ea-c7e9-4c92-878c-ca63489c66db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# dim_date_gld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e08e3eef-ff0e-4c2b-9ab5-47ba1069029f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import datetime as dt\n",
    "\n",
    "dim_date = f\"{CATALOG}.{SCHEMA}.dim_date_gld\"\n",
    "\n",
    "# Build a small date dimension around your Opp close dates\n",
    "opp_slv = f\"{CATALOG}.{SCHEMA}.anon_bst_opp_slv\"\n",
    "dfo = spark.table(opp_slv)\n",
    "\n",
    "min_dt = dfo.select(F.min(\"CloseDate\")).first()[0]\n",
    "max_dt = dfo.select(F.max(\"CloseDate\")).first()[0]\n",
    "\n",
    "if min_dt is None or max_dt is None:\n",
    "    # Fallback window if missing\n",
    "    min_dt = dt.date(2015,1,1)\n",
    "    max_dt = dt.date(2030,12,31)\n",
    "else:\n",
    "    min_dt = min_dt.date()\n",
    "    max_dt = max_dt.date()\n",
    "\n",
    "dates = []\n",
    "cur = min_dt\n",
    "while cur <= max_dt:\n",
    "    dates.append(Row(\n",
    "        date=cur,\n",
    "        year=cur.year,\n",
    "        month=cur.month,\n",
    "        day=cur.day,\n",
    "        yyyymm=int(f\"{cur.year}{cur.month:02d}\")\n",
    "    ))\n",
    "    cur += dt.timedelta(days=1)\n",
    "\n",
    "dfd = spark.createDataFrame(dates)\n",
    "(dfd.write\n",
    "    .option(\"delta.columnMapping.mode\",\"name\")\n",
    "    .option(\"overwriteSchema\",\"true\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(dim_date))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "978f181c-a35c-49dd-8285-ab9f61787144",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# fact_opportunity_gld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "542bd8b0-a3a2-44dc-8888-32c46e69e073",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "opp_slv  = f\"{CATALOG}.{SCHEMA}.anon_bst_opp_slv\"\n",
    "dim_acct = f\"{CATALOG}.{SCHEMA}.dim_account_gld\"\n",
    "fact_opp = f\"{CATALOG}.{SCHEMA}.fact_opportunity_gld\"\n",
    "\n",
    "dfo = spark.table(opp_slv)\n",
    "dfa = spark.table(dim_acct).select(\"Id\",\"account_sk\")\n",
    "\n",
    "# Choose a single amount priority\n",
    "amount_expr = F.coalesce(\n",
    "    F.col(\"Amount_ACV_USD\"),\n",
    "    F.col(\"Amount_ACV\"),\n",
    "    F.col(\"Amount_ACV_Static\")\n",
    ").alias(\"amount_acv_usd\")\n",
    "\n",
    "# Win flag\n",
    "is_won = F.when(F.lower(F.col(\"StageName\")).isin(\"closed won\",\"won\"), F.lit(1)).otherwise(F.lit(0)).alias(\"is_won\")\n",
    "\n",
    "# Build fact\n",
    "keep_raw = [c for c in dfo.columns if c in {\n",
    "    \"ID\",\"AccountID\",\"StageName\",\"CloseDate\",\"LastModifiedDate\",\"Reason_Code\",\n",
    "    \"Reporting_Org\",\"Roadmap_Indicator\",\"Record_Type_API_Name\"\n",
    "}]\n",
    "\n",
    "measure_cols = [c for c in [\"CX_Ops_Overall_ACV\",\"CX_Ops_Overall_ECR\",\"ECR_Converted\"] if c in dfo.columns]\n",
    "\n",
    "fact = (dfo\n",
    "    .select(*keep_raw, *[F.col(c) for c in measure_cols], amount_expr, is_won)\n",
    "    .join(dfa, dfo[\"AccountID\"]==dfa[\"Id\"], \"left\")\n",
    "    .withColumn(\"close_date\", F.to_date(\"CloseDate\"))\n",
    "    .withColumn(\"close_month\", F.date_format(\"close_date\",\"yyyy-MM\"))\n",
    "    .drop(\"Id\")  # from dim to avoid confusion\n",
    ")\n",
    "\n",
    "(fact.write\n",
    "    .option(\"delta.columnMapping.mode\",\"name\")\n",
    "    .option(\"overwriteSchema\",\"true\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(fact_opp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2afd9685-02a7-49eb-9e27-a6d32320eff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# agg_opportunity_monthly_gld (example mart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99b7ab12-003e-4e2e-9a43-4b3f8e4199db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fact_opp = f\"{CATALOG}.{SCHEMA}.fact_opportunity_gld\"\n",
    "agg_tbl  = f\"{CATALOG}.{SCHEMA}.agg_opportunity_monthly_gld\"\n",
    "\n",
    "f = spark.table(fact_opp)\n",
    "\n",
    "agg = (f\n",
    "  .groupBy(\"close_month\",\"Reporting_Org\")\n",
    "  .agg(\n",
    "      F.sum(\"amount_acv_usd\").alias(\"sum_acv_usd\"),\n",
    "      F.avg(\"CX_Ops_Overall_ECR\").alias(\"avg_ecr\"),\n",
    "      (F.sum(\"is_won\")/F.count(F.lit(1))).alias(\"win_rate\")\n",
    "  ))\n",
    "\n",
    "(agg.write\n",
    "   .option(\"delta.columnMapping.mode\",\"name\")\n",
    "   .option(\"overwriteSchema\",\"true\")\n",
    "   .mode(\"overwrite\")\n",
    "   .saveAsTable(agg_tbl))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_anon_gld",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
