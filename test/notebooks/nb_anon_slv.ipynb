{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c46c677d-230e-4da8-9042-d2c3c1b9c9f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, Window\n",
    "from pyspark.sql.types import DecimalType, IntegerType, TimestampType, DateType, StringType\n",
    "\n",
    "def trim_all(df):\n",
    "    # Trim all string columns and normalize whitespace\n",
    "    exprs = [\n",
    "        F.regexp_replace(F.trim(F.col(c)), r\"\\s+\", \" \").alias(c) if t == \"string\" else F.col(c)\n",
    "        for c, t in df.dtypes\n",
    "    ]\n",
    "    return df.select(*exprs)\n",
    "\n",
    "def clean_number(col):\n",
    "    # Remove currency symbols and thousands separators\n",
    "    return F.regexp_replace(\n",
    "        F.regexp_replace(F.col(col), r\"^\\s*[^\\d\\-\\.,]+\\s*\", \"\"), r\"[,\\s]\", \"\"\n",
    "    )\n",
    "\n",
    "def to_decimal(col, p=18, s=2):\n",
    "    return clean_number(col).cast(DecimalType(p, s))\n",
    "\n",
    "def to_int(col):\n",
    "    return clean_number(col).cast(IntegerType())\n",
    "\n",
    "def to_ts(col):\n",
    "    # Try common formats, fallback to Spark inference\n",
    "    return F.coalesce(\n",
    "        F.to_timestamp(F.col(col), \"M/d/yyyy H:mm\"),\n",
    "        F.to_timestamp(F.col(col), \"M/d/yyyy HH:mm\"),\n",
    "        F.to_timestamp(F.col(col), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "        F.to_timestamp(F.col(col), \"yyyy/MM/dd HH:mm:ss\"),\n",
    "        F.to_timestamp(F.col(col))  # fallback\n",
    "    )\n",
    "\n",
    "def dedupe_latest(df, key_cols, order_col):\n",
    "    # Keep latest record per key using order_col (e.g., LastModifiedDate)\n",
    "    w = Window.partitionBy(*key_cols).orderBy(F.col(order_col).desc_nulls_last())\n",
    "    return df.withColumn(\"_rn\", F.row_number().over(w)).filter(\"_rn=1\").drop(\"_rn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f816868-debf-4496-b6e1-d490151937fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"workspace\"\n",
    "SCHEMA  = \"ai_bi_reporting\"\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA  {SCHEMA}\")\n",
    "\n",
    "bz = f\"{CATALOG}.{SCHEMA}.anon_bst_acct_bz\"\n",
    "slv = f\"{CATALOG}.{SCHEMA}.anon_bst_acct_slv\"\n",
    "\n",
    "df = spark.table(bz)\n",
    "df = trim_all(df)\n",
    "\n",
    "# Type casting (based on your samples)\n",
    "df = (df\n",
    "      .withColumn(\"Id\", to_int(\"Id\"))\n",
    "      .withColumn(\"FY_Revenue_Target\", to_decimal(\"FY_Revenue_Target\", 18, 2))\n",
    "      .withColumn(\"CVA_NPS\", to_int(\"CVA_NPS\"))\n",
    "      .withColumn(\"CVA_CSAT\", to_int(\"CVA_CSAT\"))\n",
    ")\n",
    "\n",
    "# If you have a reliable modification timestamp column, dedupe by it; else, keep as-is\n",
    "# Example: no timestamp â†’ just drop exact duplicates on business key\n",
    "df = df.dropDuplicates([\"Id\"])\n",
    "\n",
    "(df.write\n",
    "   .option(\"delta.columnMapping.mode\",\"name\")\n",
    "   .option(\"overwriteSchema\",\"true\")\n",
    "   .mode(\"overwrite\")\n",
    "   .saveAsTable(slv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10f60841-ff76-4847-abd1-54ebc5af3e6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "bz = f\"{CATALOG}.{SCHEMA}.anon_bst_opp_bz\"\n",
    "slv = f\"{CATALOG}.{SCHEMA}.anon_bst_opp_slv\"\n",
    "\n",
    "df = spark.table(bz)\n",
    "df = trim_all(df)\n",
    "\n",
    "# Integers\n",
    "for c in [\n",
    "    \"ID\",\n",
    "    \"AccountID\",\n",
    "    \"Auto_Number\",\n",
    "    \"ContractRenewalMonths\"\n",
    "]:\n",
    "    if c in df.columns:\n",
    "        df = df.withColumn(\n",
    "            c,\n",
    "            expr(f\"try_cast({c} AS INT)\")\n",
    "        )\n",
    "\n",
    "# Decimals\n",
    "for c in [\n",
    "    \"Amount_ACV\",\n",
    "    \"CX_Ops_Overall_ACV\",\n",
    "    \"CX_Ops_Overall_ECR\",\n",
    "    \"Amount_ACV_Static\",\n",
    "    \"ECR_Converted\",\n",
    "    \"Amount_ACV_USD\"\n",
    "]:\n",
    "    if c in df.columns:\n",
    "        df = df.withColumn(\n",
    "            c,\n",
    "            to_decimal(c, 18, 2)\n",
    "        )\n",
    "\n",
    "# Timestamps\n",
    "for c in [\n",
    "    \"CloseDate\",\n",
    "    \"LastModifiedDate\"\n",
    "]:\n",
    "    if c in df.columns:\n",
    "        df = df.withColumn(\n",
    "            c,\n",
    "            to_ts(c)\n",
    "        )\n",
    "\n",
    "# Deduplicate by ID using LastModifiedDate if present\n",
    "order_col = \"LastModifiedDate\" if \"LastModifiedDate\" in df.columns else None\n",
    "df = dedupe_latest(df, [\"ID\"], order_col) if order_col else df.dropDuplicates([\"ID\"])\n",
    "\n",
    "(df.write\n",
    "   .option(\"delta.columnMapping.mode\", \"name\")\n",
    "   .option(\"overwriteSchema\", \"true\")\n",
    "   .mode(\"overwrite\")\n",
    "   .saveAsTable(slv)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31741c0b-d26d-485d-b04d-ddd6cdf6d924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bz = f\"{CATALOG}.{SCHEMA}.anon_cust_intelligence_opportunity_bz\"\n",
    "slv = f\"{CATALOG}.{SCHEMA}.anon_cust_intelligence_opportunity_slv\"\n",
    "\n",
    "df = spark.table(bz)\n",
    "df = trim_all(df)\n",
    "\n",
    "if \"ID\" in df.columns:\n",
    "    df = df.withColumn(\"ID\", to_int(\"ID\"))\n",
    "\n",
    "# If you later identify a better unique key, update the dedupe strategy\n",
    "df = df.dropDuplicates([\"ID\"])\n",
    "\n",
    "(df.write\n",
    "   .option(\"delta.columnMapping.mode\",\"name\")\n",
    "   .option(\"overwriteSchema\",\"true\")\n",
    "   .mode(\"overwrite\")\n",
    "   .saveAsTable(slv))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_anon_slv",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
