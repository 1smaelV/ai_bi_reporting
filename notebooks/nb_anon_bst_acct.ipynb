{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa0a4f09-534d-48d3-a38e-dc423731df73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base = \"/Volumes/advisory_demo_catalog/ai_bi_reporting/anon_files\"\n",
    "df_acct = spark.read.option(\"header\",\"true\").csv(f\"{base}/ANON BST ACCT*.csv\")\n",
    "df_opp = spark.read.option(\"header\",\"true\").csv(f\"{base}/ANON BST OPPS*.csv\")\n",
    "df_cust = spark.read.option(\"header\",\"true\").csv(f\"{base}/ANON  Cust_*.csv\")\n",
    "\n",
    "(\n",
    "    df_acct.write\n",
    "    .option(\"delta.columnMapping.mode\", \"name\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"advisory_demo_catalog.ai_bi_reporting.anon_bst_acct_bz\")\n",
    ")\n",
    "\n",
    "(\n",
    "    df_opp.write\n",
    "    .option(\"delta.columnMapping.mode\", \"name\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"advisory_demo_catalog.ai_bi_reporting.anon_bst_opp_bz\")\n",
    ")\n",
    "\n",
    "(\n",
    "    df_cust.write\n",
    "    .option(\"delta.columnMapping.mode\", \"name\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"advisory_demo_catalog.ai_bi_reporting.anon_cust_intelligence_opportunity_bz\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32174634-db81-4bf1-8300-4220fc9abf32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DecimalType, IntegerType, TimestampType\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# ==== 0) Base configuration ====\n",
    "base = \"/Volumes/advisory_demo_catalog/ai_bi_reporting/anon_files\"\n",
    "\n",
    "acct_path = f\"{base}/ANON BST ACCT*.csv\"\n",
    "opp_path  = f\"{base}/ANON BST OPPS*.csv\"\n",
    "cust_path = f\"{base}/ANON  Cust_*.csv\"\n",
    "\n",
    "CATALOG = \"advisory_demo_catalog\"\n",
    "SCHEMA  = \"ai_bi_reporting\"\n",
    "\n",
    "tbl_acct = f\"{CATALOG}.{SCHEMA}.anon_bst_acct_bz_2\"\n",
    "tbl_opp  = f\"{CATALOG}.{SCHEMA}.anon_bst_opp_bz_2\"\n",
    "tbl_cust = f\"{CATALOG}.{SCHEMA}.anon_cust_intelligence_opportunity_bz_2\"\n",
    "\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA  {SCHEMA}\")\n",
    "\n",
    "# ==== 1) Helper functions for cleaning/casting ====\n",
    "\n",
    "def clean_number(col):\n",
    "    \"\"\"\n",
    "    Remove common non-numeric characters (currency symbols, spaces, commas, etc.)\n",
    "    \"\"\"\n",
    "    return F.regexp_replace(\n",
    "        F.regexp_replace(F.col(col), r\"^\\s*[^\\d\\-\\.,]+\\s*\", \"\"),  # remove leading symbols\n",
    "        r\"[,\\s]\", \"\"                                              # remove thousand separators\n",
    "    )\n",
    "\n",
    "def to_decimal(col, precision=18, scale=2):\n",
    "    return clean_number(col).cast(DecimalType(precision, scale))\n",
    "\n",
    "def to_int(col):\n",
    "    # First cast to double, then to int, or use try_cast logic\n",
    "    return F.when(\n",
    "        F.col(col).rlike(r\"^\\s*-?\\d+(\\.\\d+)?\\s*$\"),\n",
    "        F.col(col).cast(DoubleType()).cast(IntegerType())\n",
    "    ).otherwise(None)\n",
    "\n",
    "def to_ts(col):\n",
    "    \"\"\"\n",
    "    Try to parse timestamps such as '6/22/2016 0:00' or '2024-12-31 17:00'.\n",
    "    Add additional formats if needed.\n",
    "    \"\"\"\n",
    "    return F.coalesce(\n",
    "        F.to_timestamp(F.col(col), \"M/d/yyyy H:mm\"),\n",
    "        F.to_timestamp(F.col(col), \"M/d/yyyy HH:mm\"),\n",
    "        F.to_timestamp(F.col(col), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "        F.to_timestamp(F.col(col), \"yyyy/MM/dd HH:mm:ss\"),\n",
    "        F.to_timestamp(F.col(col))  # fallback with Spark inference\n",
    "    )\n",
    "\n",
    "def to_date(col):\n",
    "    \"\"\"\n",
    "    Try to parse dates with multiple formats.\n",
    "    \"\"\"\n",
    "    return F.coalesce(\n",
    "        F.to_date(F.col(col), \"M/d/yyyy\"),\n",
    "        F.to_date(F.col(col), \"yyyy-MM-dd\"),\n",
    "        F.to_date(F.col(col), \"yyyy/MM/dd\"),\n",
    "        F.to_date(F.col(col), \"dd-MMM-yyyy\"),\n",
    "        F.to_date(F.col(col))  # fallback with Spark inference\n",
    "    )\n",
    "\n",
    "# ==== 2) Read CSV files as strings ====\n",
    "\n",
    "rd_opts = {\"header\": \"true\", \"multiLine\": \"false\", \"escape\": '\"', \"quote\": '\"', \"inferSchema\": \"false\"}\n",
    "\n",
    "df_acct_raw = spark.read.options(**rd_opts).csv(acct_path)\n",
    "df_opp_raw  = spark.read.options(**rd_opts).csv(opp_path)\n",
    "df_cust_raw = spark.read.options(**rd_opts).csv(cust_path)\n",
    "\n",
    "# ==== 3) ACCT: cast detected columns ====\n",
    "df_acct = (\n",
    "    df_acct_raw\n",
    "    .withColumn(\"Id\", to_int(\"Id\"))\n",
    "    .withColumn(\"FY_Revenue_Target\", to_decimal(\"FY_Revenue_Target\", 18, 2))\n",
    "    .withColumn(\"CVA_NPS\", to_int(\"CVA_NPS\"))\n",
    "    .withColumn(\"CVA_CSAT\", to_int(\"CVA_CSAT\"))\n",
    "    # leave other columns as string\n",
    ")\n",
    "\n",
    "# ==== 4) OPPS: cast detected columns ====\n",
    "decimal_cols_opp = [\n",
    "    \"Amount_ACV\", \"CX_Ops_Overall_ACV\", \"CX_Ops_Overall_ECR\",\n",
    "    \"Amount_ACV_Static\", \"ECR_Converted\", \"Amount_ACV_USD\"\n",
    "]\n",
    "int_cols_opp = [\"ID\", \"AccountID\", \"Auto_Number\", \"ContractRenewalMonths\"]\n",
    "\n",
    "df_opp = df_opp_raw\n",
    "for c in int_cols_opp:\n",
    "    if c in df_opp.columns:\n",
    "        df_opp = df_opp.withColumn(c, to_int(c))\n",
    "\n",
    "for c in decimal_cols_opp:\n",
    "    if c in df_opp.columns:\n",
    "        df_opp = df_opp.withColumn(c, to_decimal(c, 18, 2))\n",
    "\n",
    "for c in [\"CloseDate\", \"LastModifiedDate\"]:\n",
    "    if c in df_opp.columns:\n",
    "        df_opp = df_opp.withColumn(c, to_ts(c))\n",
    "\n",
    "# ==== 5) Cust Intelligence: cast detected columns ====\n",
    "df_cust = df_cust_raw\n",
    "if \"ID\" in df_cust.columns:\n",
    "    df_cust = df_cust.withColumn(\"ID\", to_int(\"ID\"))\n",
    "\n",
    "# ==== 6) Write to Delta using column mapping to preserve special characters ====\n",
    "\n",
    "write_opts = {\n",
    "    \"delta.columnMapping.mode\": \"name\",   # allows spaces/special characters in column names\n",
    "    \"overwriteSchema\": \"true\"\n",
    "}\n",
    "\n",
    "(df_acct.write\n",
    "    .options(**write_opts)\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(tbl_acct)\n",
    ")\n",
    "\n",
    "(df_opp.write\n",
    "    .options(**write_opts)\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(tbl_opp)\n",
    ")\n",
    "\n",
    "(df_cust.write\n",
    "    .options(**write_opts)\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(tbl_cust)\n",
    ")\n",
    "\n",
    "# ==== 7) Quick checks ====\n",
    "for t in [tbl_acct, tbl_opp, tbl_cust]:\n",
    "    print(f\"\\n===== {t} =====\")\n",
    "    spark.sql(f\"DESCRIBE EXTENDED {t}\").show(truncate=False)\n",
    "    spark.sql(f\"SELECT * FROM {t} LIMIT 5\").show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_anon_bst_acct",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
